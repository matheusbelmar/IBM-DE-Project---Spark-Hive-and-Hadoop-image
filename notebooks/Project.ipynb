{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/20 03:25:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, when, quarter, to_date, col, avg, sum as _sum\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load datasets into PySpark DataFrames\n",
    "\n",
    "Download the datasets from the folloing links using `wget` and load it in a Spark Dataframe.\n",
    "\n",
    "1. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv  \n",
    "2. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv  \n",
    "\n",
    "I used make file, and wget in bash to download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Raw_ds1=spark.read.csv(r\"file:///opt/data/imports/dataset1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Raw_ds2=spark.read.csv(r\"file:///opt/data/imports/dataset2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Display the schema of both dataframes\n",
    "\n",
    "Display the schema of `df1` and `df2` to understand the structure of the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Raw_ds1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Raw_ds2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Add a new column to each dataframe\n",
    "\n",
    "Add a new column named **year** to `df1` and **quarter** to `df2` representing the year and quarter of the data.\n",
    "\n",
    "*Hint: use withColumn. Convert the date columns which are present as string to date before extracting the year and quarter information*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Raw_ds1=\\\n",
    "    df_Raw_ds1\\\n",
    "    .withColumn(\"year\", year(to_date(\"date_column\", \"dd/MM/yyyy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----------+--------+----+\n",
      "|customer_id|date_column|amount|description|location|year|\n",
      "+-----------+-----------+------+-----------+--------+----+\n",
      "|          1|   1/1/2022|  5000| Purchase A| Store A|2022|\n",
      "|          2|  15/2/2022|  1200| Purchase B| Store B|2022|\n",
      "|          3|  20/3/2022|   800| Purchase C| Store C|2022|\n",
      "|          4|  10/4/2022|  3000| Purchase D| Store D|2022|\n",
      "|          5|   5/5/2022|  6000| Purchase E| Store E|2022|\n",
      "+-----------+-----------+------+-----------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Raw_ds1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Raw_ds2=\\\n",
    "    df_Raw_ds2\\\n",
    "    .withColumn(\"quarter\", quarter(to_date(\"transaction_date\", \"dd/MM/yyyy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+------+-------+\n",
      "|customer_id|transaction_date|value| notes|quarter|\n",
      "+-----------+----------------+-----+------+-------+\n",
      "|          1|        1/1/2022| 1500|Note 1|      1|\n",
      "|          2|       15/2/2022| 2000|Note 2|      1|\n",
      "|          3|       20/3/2022| 1000|Note 3|      1|\n",
      "|          4|       10/4/2022| 2500|Note 4|      2|\n",
      "|          5|        5/5/2022| 1800|Note 5|      2|\n",
      "+-----------+----------------+-----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Raw_ds2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Rename columns in both dataframes\n",
    "\n",
    "Rename the column **amount** to **transaction_amount** in `df1` and **value** to **transaction_value** in `df2`.\n",
    "\n",
    "*Hint: Use withColumnRenamed*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Raw_ds1=\\\n",
    "    df_Raw_ds1\\\n",
    "    .withColumnRenamed(\"amount\", \"transaction_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+-----------+--------+----+\n",
      "|customer_id|date_column|transaction_amount|description|location|year|\n",
      "+-----------+-----------+------------------+-----------+--------+----+\n",
      "|          1|   1/1/2022|              5000| Purchase A| Store A|2022|\n",
      "|          2|  15/2/2022|              1200| Purchase B| Store B|2022|\n",
      "|          3|  20/3/2022|               800| Purchase C| Store C|2022|\n",
      "|          4|  10/4/2022|              3000| Purchase D| Store D|2022|\n",
      "|          5|   5/5/2022|              6000| Purchase E| Store E|2022|\n",
      "+-----------+-----------+------------------+-----------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Raw_ds1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Raw_ds2=\\\n",
    "    df_Raw_ds2\\\n",
    "    .withColumnRenamed(\"value\", \"transaction_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------------+------+-------+\n",
      "|customer_id|transaction_date|transaction_value| notes|quarter|\n",
      "+-----------+----------------+-----------------+------+-------+\n",
      "|          1|        1/1/2022|             1500|Note 1|      1|\n",
      "|          2|       15/2/2022|             2000|Note 2|      1|\n",
      "|          3|       20/3/2022|             1000|Note 3|      1|\n",
      "|          4|       10/4/2022|             2500|Note 4|      2|\n",
      "|          5|        5/5/2022|             1800|Note 5|      2|\n",
      "+-----------+----------------+-----------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Raw_ds2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Drop unnecessary columns\n",
    "\n",
    "Drop the columns **description** and **location** from `df1` and **notes** from `df2`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided to drop the raw df here, for no specific reason other\n",
    "# than realizing I was still using the raw naming\n",
    "\n",
    "df_1 = df_Raw_ds1.drop(\"description\", 'location')\n",
    "df_2 = df_Raw_ds2.drop(\"note\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Join dataframes based on a common column\n",
    "\n",
    "Join `df1` and `df2` based on the common column **customer_id** and create a new dataframe named `joined_df`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_1.join(df_2, on=\"customer_id\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value| notes|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+------+-------+\n",
      "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|Note 1|      1|\n",
      "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|Note 2|      1|\n",
      "|          3|  20/3/2022|               800|2022|       20/3/2022|             1000|Note 3|      1|\n",
      "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|Note 4|      2|\n",
      "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|Note 5|      2|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=\\\n",
    "    joined_df\\\n",
    "    .filter(col(\"transaction_amount\")>1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value| notes|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+------+-------+\n",
      "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|Note 1|      1|\n",
      "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|Note 2|      1|\n",
      "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|Note 4|      2|\n",
      "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|Note 5|      2|\n",
      "|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200|Note 6|      2|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_amount_by_customer=\\\n",
    "    filtered_df\\\n",
    "    .groupBy(\"customer_id\")\\\n",
    "    .agg(_sum(\"transaction_amount\").alias(\"Total_Transactions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|Total_Transactions|\n",
      "+-----------+------------------+\n",
      "|         31|              3200|\n",
      "|         85|              1800|\n",
      "|         78|              1500|\n",
      "|         34|              1200|\n",
      "|         81|              5500|\n",
      "+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Agg_amount_by_customer.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10: Write the filtered data to HDFS\n",
    "\n",
    "Write `filtered_df` to HDFS in parquet format to a file named **filtered_data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+\n",
      "|customer_id|date_column|transaction_amount|year|\n",
      "+-----------+-----------+------------------+----+\n",
      "|          1|   1/1/2022|              5000|2022|\n",
      "|          2|  15/2/2022|              1200|2022|\n",
      "|          3|  20/3/2022|               800|2022|\n",
      "|          4|  10/4/2022|              3000|2022|\n",
      "|          5|   5/5/2022|              6000|2022|\n",
      "+-----------+-----------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Add a new column based on a condition\n",
    "\n",
    "Add a new column named **high_value** to `df1` indicating whether the transaction_amount is greater than 5000. When the value is greater than 5000, the value of the column should be **Yes**. When the value is less than or equal to 5000, the value of the column should be **No**. \n",
    "\n",
    "*Hint: Use when and lit from pyspark.sql.functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=\\\n",
    "     df_1\\\n",
    "    .withColumn(\"high_value\", when(col(\"transaction_amount\") > 5000, \"Yes\").otherwise(\"No\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------+\n",
      "|customer_id|date_column|transaction_amount|year|high_value|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "|          1|   1/1/2022|              5000|2022|        No|\n",
      "|          2|  15/2/2022|              1200|2022|        No|\n",
      "|          3|  20/3/2022|               800|2022|        No|\n",
      "|          4|  10/4/2022|              3000|2022|        No|\n",
      "|          5|   5/5/2022|              6000|2022|       Yes|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Calculate the average transaction value per quarter\n",
    "\n",
    "Calculate and display the average transaction value for each quarter in `df2` and create a new dataframe named `average_value_per_quarter` with column `avg_trans_val`.\n",
    "\n",
    "*Hint: Use avg from pyspark.sql.functions*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value_per_quarter=\\\n",
    "    df_2\\\n",
    "    .withColumn(\"year\", year(to_date(col(\"transaction_date\"), \"dd/MM/yyyy\")))\\\n",
    "    .groupBy(\"year\", \"quarter\")\\\n",
    "    .agg(avg(col(\"transaction_value\")).alias('avg_trans_val'))\\\n",
    "    .select(\"year\", \"quarter\", \"avg_trans_val\")\\\n",
    "    .orderBy(col(\"year\").asc(), col(\"quarter\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|year|quarter|     avg_trans_val|\n",
      "+----+-------+------------------+\n",
      "|2022|      1|            1500.0|\n",
      "|2022|      2|1833.3333333333333|\n",
      "|2022|      3|1433.3333333333333|\n",
      "|2022|      4|1166.6666666666667|\n",
      "|2023|      1|            1500.0|\n",
      "+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_value_per_quarter.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/20 03:25:45 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    }
   ],
   "source": [
    "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 14: Calculate the total transaction value per year\n",
    "\n",
    "Calculate and display the total transaction value for each year in `df1` and create a new dataframe named `total_value_per_year` with column `total_transaction_val`.\n",
    "> **Note:** The provided DataFrame `df1` does not explicitly have a `year` column initially. However, in Task 3, a new column named `year` is added to `df1` by extracting the year from the date column. Additionally, in Task 4, the column `amount` is renamed to `transaction_amount`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_per_year=\\\n",
    "     df_1\\\n",
    "    .groupBy(\"year\")\\\n",
    "    .agg(_sum(\"transaction_amount\").alias(\"total_transaction_val\"))\\\n",
    "    .orderBy(col(\"year\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|year|total_transaction_val|\n",
      "+----+---------------------+\n",
      "|2022|                29800|\n",
      "|2023|                28100|\n",
      "|2024|                25700|\n",
      "|2025|                25700|\n",
      "|2026|                25700|\n",
      "+----+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_value_per_year.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 15: Write the result to HDFS\n",
    "\n",
    "Write `total_value_per_year` to HDFS in the CSV format to file named **total_value_per_year**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the tables to check if it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|  default|quarterly_averages|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|year|quarter|     avg_trans_val|\n",
      "+----+-------+------------------+\n",
      "|2022|      1|            1500.0|\n",
      "|2022|      2|1833.3333333333333|\n",
      "|2022|      3|1433.3333333333333|\n",
      "|2022|      4|1166.6666666666667|\n",
      "|2023|      1|            1500.0|\n",
      "+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive_table=spark.table(\"quarterly_averages\")\n",
    "hive_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|year                        |int                                                           |NULL   |\n",
      "|quarter                     |int                                                           |NULL   |\n",
      "|avg_trans_val               |double                                                        |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |default                                                       |       |\n",
      "|Table                       |quarterly_averages                                            |       |\n",
      "|Owner                       |root                                                          |       |\n",
      "|Created Time                |Wed Aug 20 03:25:45 UTC 2025                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.1                                                   |       |\n",
      "|Type                        |MANAGED                                                       |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Statistics                  |1209 bytes                                                    |       |\n",
      "|Location                    |hdfs://namenode:9000/user/hive/warehouse/quarterly_averages   |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED quarterly_averages\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+-------+\n",
      "|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|  notes|quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+-------+\n",
      "|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500| Note 1|      1|\n",
      "|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000| Note 2|      1|\n",
      "|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500| Note 4|      2|\n",
      "|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800| Note 5|      2|\n",
      "|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200| Note 6|      2|\n",
      "|          8|  20/8/2022|              3500|2022|       20/8/2022|             3000| Note 8|      3|\n",
      "|         10| 30/10/2022|              1800|2022|      30/10/2022|             1200|Note 10|      4|\n",
      "|         11|  5/11/2022|              2200|2022|       5/11/2022|             1500|Note 11|      4|\n",
      "|         13|  15/1/2023|              4800|2023|       15/1/2023|             2000|Note 13|      1|\n",
      "|         15|  25/3/2023|              4200|2023|       25/3/2023|             1800|Note 15|      1|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"filtered_data.parquet\")\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
