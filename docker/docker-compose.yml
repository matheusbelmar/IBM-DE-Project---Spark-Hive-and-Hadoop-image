services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870" 
      - "9000:9000" 
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    ports:
      - "8088:8088"
    environment:
      - SERVICE_PRECONDITION=namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      - SERVICE_PRECONDITION=resourcemanager:8088
    env_file:
      - ./hadoop.env
    depends_on:
      - resourcemanager

  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    ports:
      - "8080:8080" 
      - "7077:7077" 
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_MASTER_PORT=7077
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf
    volumes:
      - ./conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./conf/core-site.xml:/opt/bitnami/spark/conf/core-site.xml:ro  
      - ./hive/conf/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml:ro
      - ../data:/opt/data 
      - ../spark-apps:/opt/spark-apps 
    depends_on:
      - namenode
      - datanode
      - resourcemanager 

  spark-worker:
    image: bitnami/spark:3.5.1
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2 
      - SPARK_WORKER_MEMORY=4G 
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf
    volumes:
      - ./conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./conf/core-site.xml:/opt/bitnami/spark/conf/core-site.xml:ro  
      - ./hive/conf/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml:ro
      - ../data:/opt/data
      - ../spark-apps:/opt/spark-apps
    depends_on:
      - spark-master

  python:
    build:
      context: ./python-app 
      dockerfile: Dockerfile
    image: my-python-app
    container_name: python
    volumes:
      - ../spark-apps:/opt/spark-apps 
      - ../data:/opt/data 
      - ../notebooks:/opt/notebooks
      - ../Makefile:/opt/Makefile
      - ../bash-scripts:/opt/bash-scripts
    working_dir: /opt/spark-apps
    command: tail -f /dev/null 
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
      - spark-worker 

  postgresql:
    image: postgres:13.9
    container_name: postgresql
    restart: always
    environment:
      POSTGRES_DB: hive
      POSTGRES_USER: hiveuser
      POSTGRES_PASSWORD: hivepassword
    ports:
      - "5433:5432" 
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./conf/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./conf/pg_hba.conf:/etc/postgresql/pg_hba.conf

  hive-metastore:
    build: 
      context: ./hive
      dockerfile: Metastore.Dockerfile
    image: my-hive-metastore:3.1.3
    container_name: hive-metastore
    restart: "always"
    ports:
      - "9083:9083"
    environment:
      DB_DRIVER: postgres 
      SERVICE_PRECONDITION: namenode:9000 postgresql:5432
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://postgresql:5432/hive"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionUserName: "hiveuser"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionPassword: "hivepassword"
    depends_on:
      - namenode
      - datanode
      - postgresql

  hive-server:
    build:
      context: ./hive
      dockerfile: Server.Dockerfile
    image: my-hive-server:3.1.3
    container_name: hive-server
    restart: always
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      DB_DRIVER: postgres 
      SERVICE_PRECONDITION: hive-metastore:9083
      HIVE_SITE_CONF_hive.metastore.uris: "thrift://hive-metastore:9083"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
    depends_on:
      - hive-metastore

volumes:
  hadoop_namenode:
  hadoop_datanode:
  postgres_data: